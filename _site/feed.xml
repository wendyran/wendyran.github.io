<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Versatile Spirit</title>
    <subtitle>Ran Wei&#39;s Blog</subtitle>
    <description>Welcome to my website! If you have any questions or comments, send me an email (versatile.spirit.info@gmail.com).</description>
    <link>http://wendyran.github.io/</link>
    <atom:link href="http://wendyran.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Wed, 27 May 2015 18:37:47 -0400</pubDate>
    <lastBuildDate>Wed, 27 May 2015 18:37:47 -0400</lastBuildDate>
    <generator>Jekyll v2.5.3</generator>
    
      <item>
        <title>A Closer Look at Decision Trees and Random Forests</title>
        <description>&lt;h2 id=&quot;decision-tree&quot;&gt;Decision Tree&lt;/h2&gt;

&lt;h4 id=&quot;how-decision-tree-works&quot;&gt;1. How Decision Tree Works&lt;/h4&gt;

&lt;p&gt;The definition of a decision tree (or classification &amp;amp; regression tree) procedure is:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Beginning with a parent node which contains the entire sample, the C&amp;amp;RT procedure iteratively examines all available independent, or splitting, variables, selects the one that results in binary groups that are most different with respect to the dependent variable, according to a predetermined splitting criterion and stops when the stopping rule is satisfied. At the point that no further split is made, a terminal node is created.&lt;/em&gt;[1]&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Splitting Criterion&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Node Impurity, or Diversity Index, denoted as &lt;script type=&quot;math/tex&quot;&gt;Q_m(T)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Improvement Measure of Node Impurity.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Regression Tree&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Squared-error Impurity: Sum of squared devitaions about the mean.&lt;/p&gt;

&lt;p&gt;The node predicts the sample mean of Y, which yields piecewise constant models.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q_m(T) = \frac{1}{N_m}\sum_{x_i \in R_m}(y_i-\hat{c}_m)^2&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;Classification Tree&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Define the proportion of class &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; observations in node &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}_{mk}
=\frac{1}{N_m}\sum_{x_i \in R_m}I(y_i=k)&lt;/script&gt;

&lt;p&gt;We classify the observations in node &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; to class &lt;script type=&quot;math/tex&quot;&gt;k(m)=argmax_k\hat{p}_{km}&lt;/script&gt;, the majority class in node &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Gini Index:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q_m(T)=\sum_{k\neq k&#39;}\hat{p}_{mk}\hat{p}_{mk&#39;}=\sum_{k=1}^K\hat{p}_{mk}(1-\hat{p}_{mk})&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Misclassification Error:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q_m(T)=\frac{1}{N_m}\sum_{i\in R_m}I(y_i\neq k(m))=1-\hat{p}_{k(m),m}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Cross-entropy or Deviance:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q_m(T)=\sum_{k=1}^K\hat{p}_{mk}log\hat{p}_{mk}&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;Compute Improvement Measure&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Take Gini Index as an example [1]&lt;/p&gt;

&lt;p&gt;– 1. The first step involves calculating Gini impurity function for the parent node, which is sometimes referred to as the Gini diversity index. (Diversity index)&lt;/p&gt;

&lt;p&gt;– 2. The second step involves calculating the Gini diversity index for each of the two child nodes into which the parent node splits. (diversity index1 and diversity index2)&lt;/p&gt;

&lt;p&gt;– 3. The third step involves calculating the weighted average, according to the proportion of the parent node that is included in each child node, of the Gini diversity indexes for each of the child nodes. This can be obtained by solving the following equation:&lt;/p&gt;

&lt;p&gt;Weighted diversity index = [(p1)(diversity index1)] + [(p2)(diversity index2)],&lt;/p&gt;

&lt;p&gt;where p1 and p2 refer to the proportions of the parent node that are included in the respective child nodes.&lt;/p&gt;

&lt;p&gt;– 4. The last step requires calculating the Gini improvement measure, which is equal to the following:&lt;/p&gt;

&lt;p&gt;Improvement measure = diversity index of parent node weighted diversity index.&lt;/p&gt;

&lt;p&gt;Larger values of the Gini improvement measure indicate greater difference with respect to the prevalence of the dependent measure in the two child nodes. The independent variable whose split provides the largest value of the improvement measure is selected for splitting at each step.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Illustrating Example&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Stopping Rule&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Define the minimum number of individuals in the child node or in the terminal nodes&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Define the maximum number of levels to which the tree can grow.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Predetermine the minimum value of the splitting criterion.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;related-algorithms-2&quot;&gt;2. Related Algorithms [2]&lt;/h4&gt;

&lt;blockquote&gt;
  &lt;p&gt;Classification Tree&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The first published classification tree algorithm is &lt;strong&gt;THAID&lt;/strong&gt;. Employing a measure of node impurity based on the distribution of the observed Y values in the node, THAID splits a node by exhaustively searching over all X and S for the split {X ∈ S} that minimizes the total impurity of its two child nodes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;C4.5&lt;/strong&gt; and &lt;strong&gt;CART&lt;/strong&gt; are two later classification tree algorithms that follow this approach. C4.5 uses entropy for its impurity function, whereas CART uses a generalization of the binomial variance called the Gini index.&lt;/p&gt;

&lt;p&gt;Unlike THAID, however, they first grow an overly large tree and then prune it to a smaller size to minimize an estimate of the misclassi- fication error. CART employs 10-fold (default) cross- validation, whereass C4.5 uses a heuristic formula to estimate error rates. CART is implemented in the R system as RPART which we use in the examples below.&lt;/p&gt;

&lt;p&gt;Building on an idea that originated in the &lt;strong&gt;FACT&lt;/strong&gt; algorithm, &lt;strong&gt;CRUISE&lt;/strong&gt;, &lt;strong&gt;GUIDE&lt;/strong&gt; and &lt;strong&gt;QUEST&lt;/strong&gt; use a two-step approach based on significance tests to split each node. &lt;strong&gt;CHAID&lt;/strong&gt; uses significance tests and Bonferroni corrections to try to iteratively merge pairs of child nodes.&lt;/p&gt;

&lt;p&gt;A comparision talbe is listed in [2].&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Regression Tree&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Historically, the first regression tree algorithm is &lt;strong&gt;AID&lt;/strong&gt; which appeared several years before THAID. The AID and CART regression tree methods follow Algorithm 1, with the node impurity being the sum of squared deviations about the mean and the node predicting the sample mean of Y.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;M5&lt;/strong&gt;, an adaptation of a regression tree algorithm by Quinlan uses a more computationally efficient strategy to construct piecewise linear models.&lt;/p&gt;

&lt;p&gt;A comparision table of CART, GUIDE and M5 is listed in [2].&lt;/p&gt;

&lt;h4 id=&quot;properties-of-decision-tree-tbc&quot;&gt;3. Properties of Decision Tree (TBC)&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Deal with interactions among independent variables naturally. In the regression setting, it’s hard to interpret interactions when there are more than 2 variables.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If X takes ordered values, the set S is an interval of the form (−∞, c]. Otherwise, S is a subset of the values taken by X. The process is applied recursively on the data in each child node. Splitting stops if the relative decrease in impurity is below a prespecified threshold.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;random-forest&quot;&gt;Random Forest&lt;/h2&gt;

&lt;p&gt;(Coming Soon)&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Wondering how I typed in the beautiful equations using Jekyll? These are two helpful posts that tell you how to use MathJex with Jekyll:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;http://docs.mathjax.org/en/latest/start.html&quot;&gt;MathJex&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://gastonsanchez.com/blog/opinion/2014/02/16/Mathjax-with-jekyll.html&quot;&gt;MathJex with Jekyll&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Pandoc, on the other way, is a poweful tool to &lt;a href=&quot;http://kesdev.com/you-got-latex-in-my-markdown/&quot;&gt;use LaTex in Markdown&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;References:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Lemon, Stephenie C., et al. “Classification and regression tree analysis in public health: methodological review and comparison with logistic regression.” Annals of behavioral medicine 26.3 (2003): 172-181.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Loh, Wei‐Yin. “Classification and regression trees.” Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 1.1 (2011): 14-23.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Tue, 26 May 2015 14:04:00 -0400</pubDate>
        <link>http://wendyran.github.io/jekyll/update/2015/05/26/decision-tree-random-forest-svm.html</link>
        <guid isPermaLink="true">http://wendyran.github.io/jekyll/update/2015/05/26/decision-tree-random-forest-svm.html</guid>
        
        
        <category>jekyll</category>
        
        <category>update</category>
        
      </item>
    
      <item>
        <title>My Blog History and Why I Blog</title>
        <description>&lt;p&gt;I started my first personal blog using &lt;a href=&quot;http://spaces.live.com&quot;&gt;MSN Space&lt;/a&gt; (or Windows Live Spaces, shut down in 2011) starting summer 2006. It was after I graduated from my &lt;a href=&quot;http://zh.wikipedia.org/wiki/%E5%8D%8E%E4%B8%AD%E5%B8%88%E8%8C%83%E5%A4%A7%E5%AD%A6%E7%AC%AC%E4%B8%80%E9%99%84%E5%B1%9E%E4%B8%AD%E5%AD%A6&quot;&gt;high school&lt;/a&gt; and before I went to &lt;a href=&quot;http://en.wikipedia.org/wiki/Zhejiang_University&quot;&gt;Zhejiang University&lt;/a&gt; for my undergraduate study. The MSN Space is where I kept a record of my thoughts after the stressful high school and before I entered my colorful college life. Since then, like many others, I’ve written in several different blogs: &lt;a href=&quot;http://www.ycool.com&quot;&gt;ycool&lt;/a&gt;(previously ‘yculblog’; Chinese name: 歪酷), &lt;a href=&quot;http://blog.sina.com&quot;&gt;sina blog&lt;/a&gt;, &lt;a href=&quot;http://hi.baidu.com&quot;&gt;baidu space&lt;/a&gt; (closed on May 7th, 2015) and &lt;a href=&quot;http://wordpress.com&quot;&gt;wordpress&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;During my graduate school at the Ohio State University where I’m pursuing my Ph.D. in &lt;a href=&quot;http://www.stat.osu.edu&quot;&gt;Statistics&lt;/a&gt;, I continued my enthusiasm for blogging. I have been updating my offline blog all the time but it was only until recently that I really had a chance to start building my own blog website, with the help from &lt;a href=&quot;http://pages.github.com&quot;&gt;github pages&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I had a natural urge to record the ever past moments and the exciting thoughts that ever hit me. Time passes so fast yet there were so many things you wanted to carve into something that would never vanish. This is the general reason of me blogging. While humans’ thoughts can be so subtle that typing speed could hardly catch up with the flow, I still would like to try my best. Or else most things would just pass by without a clue of ever being existed.&lt;/p&gt;

&lt;p&gt;English is not my native language. Even though I started learning it at an early age and have lived in the U.S. for almost 5 years, I still want to keep improving my English skills. Writing blogs, on the other hand, would definitely help a lot, not to mention how it would contribute to my Ph.D. dissertation writing.&lt;/p&gt;

&lt;p&gt;As I have mentioned, statistics is my major. So you are going to see lots of posts about data science and statistics in this blog. I enjoy taking photographs as well so this is another category you are going to see in this website.&lt;/p&gt;

</description>
        <pubDate>Tue, 26 May 2015 05:00:00 -0400</pubDate>
        <link>http://wendyran.github.io/jekyll/update/2015/05/26/why-i-blog-and-my-blog-history.html</link>
        <guid isPermaLink="true">http://wendyran.github.io/jekyll/update/2015/05/26/why-i-blog-and-my-blog-history.html</guid>
        
        
        <category>jekyll</category>
        
        <category>update</category>
        
      </item>
    
  </channel>
</rss>
