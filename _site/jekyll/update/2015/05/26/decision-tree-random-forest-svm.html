<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>A Closer Look at Decision Trees and Random Forests</title>
  <meta name="description" content="Decision Tree">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://wendyran.github.io/jekyll/update/2015/05/26/decision-tree-random-forest-svm.html">
  <link rel="alternate" type="application/rss+xml" title="Versatile Spirit" href="http://wendyran.github.io/feed.xml" />
</head>


  <body>
  
  <script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
  
    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Versatile Spirit</a>
    <br>
    <p class="site-subtitle">Ran Wei's Blog</p>
    
    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
          <a class="page-link" href="/about/">About Me</a>
          
        
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="post">
  <script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
   
  <header class="post-header">
    <h1 class="post-title">A Closer Look at Decision Trees and Random Forests</h1>
    <p class="post-meta">May 26, 2015</p>
  </header>

  <article class="post-content">
    <h2 id="decision-tree">Decision Tree</h2>

<h4 id="how-decision-tree-works">1. How Decision Tree Works</h4>

<p>The definition of a decision tree (or classification &amp; regression tree) procedure is:</p>

<p><em>Beginning with a parent node which contains the entire sample, the C&amp;RT procedure iteratively examines all available independent, or splitting, variables, selects the one that results in binary groups that are most different with respect to the dependent variable, according to a predetermined splitting criterion and stops when the stopping rule is satisfied. At the point that no further split is made, a terminal node is created.</em>[1]</p>

<p><strong>Splitting Criterion</strong></p>

<p>Node Impurity, or Diversity Index, denoted as <script type="math/tex">Q_m(T)</script>.</p>

<p>Improvement Measure of Node Impurity.</p>

<blockquote>
  <p>Regression Tree</p>
</blockquote>

<p>Squared-error Impurity: Sum of squared devitaions about the mean.</p>

<p>The node predicts the sample mean of Y, which yields piecewise constant models.</p>

<script type="math/tex; mode=display">Q_m(T) = \frac{1}{N_m}\sum_{x_i \in R_m}(y_i-\hat{c}_m)^2</script>

<blockquote>
  <p>Classification Tree</p>
</blockquote>

<p>Define the proportion of class <script type="math/tex">k</script> observations in node <script type="math/tex">m</script>.</p>

<script type="math/tex; mode=display">\hat{p}_{mk}
=\frac{1}{N_m}\sum_{x_i \in R_m}I(y_i=k)</script>

<p>We classify the observations in node <script type="math/tex">m</script> to class <script type="math/tex">k(m)=argmax_k\hat{p}_{km}</script>, the majority class in node <script type="math/tex">m</script>.</p>

<ul>
  <li>Gini Index:</li>
</ul>

<script type="math/tex; mode=display">Q_m(T)=\sum_{k\neq k'}\hat{p}_{mk}\hat{p}_{mk'}=\sum_{k=1}^K\hat{p}_{mk}(1-\hat{p}_{mk})</script>

<ul>
  <li>Misclassification Error:</li>
</ul>

<script type="math/tex; mode=display">Q_m(T)=\frac{1}{N_m}\sum_{i\in R_m}I(y_i\neq k(m))=1-\hat{p}_{k(m),m}</script>

<ul>
  <li>Cross-entropy or Deviance:</li>
</ul>

<script type="math/tex; mode=display">Q_m(T)=\sum_{k=1}^K\hat{p}_{mk}log\hat{p}_{mk}</script>

<blockquote>
  <p>Compute Improvement Measure</p>
</blockquote>

<p>Take Gini Index as an example [1]</p>

<p>– 1. The first step involves calculating Gini impurity function for the parent node, which is sometimes referred to as the Gini diversity index. (Diversity index)</p>

<p>– 2. The second step involves calculating the Gini diver- sity index for each of the two child nodes into which the parent node splits. (diversity index1 and diversity index2)</p>

<p>– 3. The third step involves calculating the weighted aver- age, according to the proportion of the parent node that is included in each child node, of the Gini diver- sity indexes for each of the child nodes. This can be obtained by solving the following equation:</p>

<p>Weighted diversity index = [(p1)(diversity index1)] + [(p2)(diversity index2)],</p>

<p>where p1 and p2 refer to the proportions of the parent node that are included in the respective child nodes.</p>

<p>– 4. The last step requires calculating the Gini improve- ment measure, which is equal to the following:</p>

<p>Improvement measure = diversity index of parent node – weighted diversity index.</p>

<p>Larger values of the Gini improvement measure indicate greater difference with respect to the prevalence of the depend- ent measure in the two child nodes. The independent variable whose split provides the largest value of the improvement measure is selected for splitting at each step.</p>

<blockquote>
  <p>Illustrating Example</p>
</blockquote>

<p><strong>Stopping Rule</strong></p>

<ul>
  <li>
    <p>Define the minimum number of individuals in the child node or in the terminal nodes</p>
  </li>
  <li>
    <p>Define the maximum number of levels to which the tree can grow.</p>
  </li>
  <li>
    <p>Predetermine the minimum value of the splitting criterion.</p>
  </li>
</ul>

<h4 id="related-algorithms-2">2. Related Algorithms [2]</h4>

<blockquote>
  <p>Classification Tree</p>
</blockquote>

<p>The first published classification tree algorithm is <strong>THAID</strong>. Employing a measure of node impurity based on the distribution of the observed Y values in the node, THAID splits a node by exhaustively searching over all X and S for the split {X ∈ S} that minimizes the total impurity of its two child nodes.</p>

<p><strong>C4.5</strong> and <strong>CART</strong> are two later classification tree algorithms that follow this approach. C4.5 uses entropy for its impurity function, whereas CART uses a generalization of the binomial variance called the Gini index.</p>

<p>Unlike THAID, however, they first grow an overly large tree and then prune it to a smaller size to minimize an estimate of the misclassi- fication error. CART employs 10-fold (default) cross- validation, whereass C4.5 uses a heuristic formula to estimate error rates. CART is implemented in the R system as RPART which we use in the examples below.</p>

<p>Building on an idea that originated in the <strong>FACT</strong> algorithm, <strong>CRUISE</strong>, <strong>GUIDE</strong> and <strong>QUEST</strong> use a two-step approach based on significance tests to split each node. <strong>CHAID</strong> uses significance tests and Bonferroni corrections to try to iteratively merge pairs of child nodes.</p>

<p>A comparision talbe is listed in [2].</p>

<blockquote>
  <p>Regression Tree</p>
</blockquote>

<p>Historically, the first regression tree algorithm is <strong>AID</strong> which appeared several years before THAID. The AID and CART regression tree methods follow Algorithm 1, with the node impurity being the sum of squared deviations about the mean and the node predicting the sample mean of Y.</p>

<p><strong>M5</strong>, an adaptation of a regression tree al- gorithm by Quinlan uses a more computationally efficient strategy to construct piecewise linear models.</p>

<p>A comparision table of CART, GUIDE and M5 is listed in [2].</p>

<h4 id="properties-of-decision-tree-tbc">3. Properties of Decision Tree (TBC)</h4>

<ul>
  <li>
    <p>Deal with interactions among independent variables naturally. In the regression setting, it’s hard to interpret interactions when there are more than 2 variables.</p>
  </li>
  <li>
    <p>If X takes ordered values, the set S is an interval of the form (−∞, c]. Otherwise, S is a subset of the values taken by X. The process is applied recursively on the data in each child node. Splitting stops if the relative decrease in impurity is below a prespecified threshold.</p>
  </li>
</ul>

<h2 id="random-forest">Random Forest</h2>

<hr />
<p>Wondering how I typed in the beautiful equations using Jekyll? These are two helpful posts that tell you how to use MathJex with Jekyll:</p>

<ol>
  <li><a href="http://docs.mathjax.org/en/latest/start.html">MathJex</a></li>
  <li><a href="http://gastonsanchez.com/blog/opinion/2014/02/16/Mathjax-with-jekyll.html">MathJex with Jekyll</a></li>
</ol>

<p>Pandoc, on the other way, is a poweful tool to <a href="http://kesdev.com/you-got-latex-in-my-markdown/">use LaTex in Markdown</a>.</p>

<hr />
<p>References:</p>

<ol>
  <li>
    <p>Lemon, Stephenie C., et al. “Classification and regression tree analysis in public health: methodological review and comparison with logistic regression.” Annals of behavioral medicine 26.3 (2003): 172-181.</p>
  </li>
  <li>
    <p>Loh, Wei‐Yin. “Classification and regression trees.” Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 1.1 (2011): 14-23.</p>
  </li>
</ol>


  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Versatile Spirit</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col  footer-col-1">
        <ul class="contact-list">
          <li>Versatile Spirit</li>
          <li><a href="mailto:versatile.spirit.info@gmail.com">versatile.spirit.info@gmail.com</a></li>
        </ul>
      </div>

      <div class="footer-col  footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/wendyran">
              <span class="icon  icon--github">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
                </svg>
              </span>

              <span class="username">wendyran</span>
            </a>
          </li>
          

          
          <li>
            <a href="https://twitter.com/wendyranstat">
              <span class="icon  icon--twitter">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                  c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
                </svg>
              </span>

              <span class="username">wendyranstat</span>
            </a>
          </li>
          
        </ul>
      </div>

      <div class="footer-col  footer-col-3">
        <p class="text">Welcome to my website! 
</p>
      </div>
    </div>

    <p> Copyright &copy; 2015 - Ran Wei - Powered by  
      <a href="http://jekyllrb.com/">Jekyll
      </a>
    </p>

  </div>

</footer>


  </body>

</html>
